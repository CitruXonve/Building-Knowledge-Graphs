{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `AmpliGraph` to generate GoT Knowledge Graph Embeddings\n",
    "\n",
    "<sub>Content of this notebook was prepared by Basel Shbita (shbita@usc.edu) as part of the class <u>CSCI 563/INF 558: Building Knowledge Graphs</u> during Spring 2020 at University of Southern California (USC).</sub>\n",
    "\n",
    "**Notes**: \n",
    "- You are supposed to write your code or modify our code in any cell starting with `# ** STUDENT CODE`.\n",
    "- Much content of this notebook was borrowed from AmpliGraph tutorials"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AmpliGraph` is a suite of neural machine learning models for relational learning, a branch of machine learning that deals with supervised learning on knowledge graphs. It can be used to <u>generate stand-alone knowledge graph embeddings</u>, discover new knowledge from an existing knowledge graph and complete large knowledge graphs with missing statements."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this task, you will gain some hands-on experience working with Knowledge Graph Embeddings. Specifically, you will use the *ComplEx* model to learn the embeddings of a (small) KG. You will be required to split the dataset to train and test sets, train the model, evaluate it and then generate a visualization!**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "\n",
    "Lets install the packages we will use"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: tensorflow<2.0,>=1.14.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.15.2)\nRequirement already satisfied: scipy in /Users/crxon/558/env/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.4.1)\nRequirement already satisfied: numpy in /Users/crxon/558/env/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.18.1)\nRequirement already satisfied: pandas in /Users/crxon/558/env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.0.1)\nRequirement already satisfied: ampligraph in /Users/crxon/558/env/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.3.1)\nRequirement already satisfied: six>=1.10.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.14.0)\nRequirement already satisfied: termcolor>=1.1.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.1.0)\nRequirement already satisfied: astor>=0.6.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (0.8.1)\nRequirement already satisfied: tensorflow-estimator==1.15.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.15.1)\nRequirement already satisfied: keras-applications>=1.0.8 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.0.8)\nRequirement already satisfied: google-pasta>=0.1.6 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (0.2.0)\nRequirement already satisfied: gast==0.2.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (0.2.2)\nRequirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.15.0)\nRequirement already satisfied: wrapt>=1.11.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.12.1)\nRequirement already satisfied: grpcio>=1.8.6 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.28.1)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (0.34.2)\nRequirement already satisfied: protobuf>=3.6.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (3.11.3)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.1.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: absl-py>=0.7.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (0.9.0)\nRequirement already satisfied: pytz>=2017.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 4)) (2019.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 4)) (2.8.1)\nRequirement already satisfied: recommonmark>=0.4.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (0.6.0)\nRequirement already satisfied: deap>=1.2.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (1.3.1)\nRequirement already satisfied: pytest>=3.5.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (5.4.1)\nRequirement already satisfied: sphinx>=2.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (3.0.2)\nRequirement already satisfied: rdflib>=4.2.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (4.2.2)\nRequirement already satisfied: beautifultable>=0.7.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (0.8.0)\nRequirement already satisfied: networkx>=2.3 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (2.4)\nRequirement already satisfied: flake8>=3.7.7 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (3.7.9)\nRequirement already satisfied: joblib>=0.11 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (0.14.1)\nRequirement already satisfied: sphinxcontrib-bibtex>=0.4.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (1.0.0)\nRequirement already satisfied: sphinx-rtd-theme>=0.4.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (0.4.3)\nRequirement already satisfied: scikit-learn>=0.19.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (0.22.2.post1)\nRequirement already satisfied: tqdm>=4.23.4 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (4.43.0)\nRequirement already satisfied: pyyaml>=3.13 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (5.3.1)\nRequirement already satisfied: setuptools>=36 in /Users/crxon/558/env/lib/python3.7/site-packages (from ampligraph->-r requirements.txt (line 5)) (46.1.3)\nRequirement already satisfied: h5py in /Users/crxon/558/env/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (2.10.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (1.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /Users/crxon/558/env/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14.0->-r requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: docutils>=0.11 in /Users/crxon/558/env/lib/python3.7/site-packages (from recommonmark>=0.4.0->ampligraph->-r requirements.txt (line 5)) (0.16)\nRequirement already satisfied: commonmark>=0.8.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from recommonmark>=0.4.0->ampligraph->-r requirements.txt (line 5)) (0.9.1)\nRequirement already satisfied: attrs>=17.4.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (19.3.0)\nRequirement already satisfied: wcwidth in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (0.1.8)\nRequirement already satisfied: py>=1.5.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (1.8.1)\nRequirement already satisfied: packaging in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (20.3)\nRequirement already satisfied: more-itertools>=4.0.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (8.2.0)\nRequirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (1.5.0)\nRequirement already satisfied: pluggy<1.0,>=0.12 in /Users/crxon/558/env/lib/python3.7/site-packages (from pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (0.13.1)\nRequirement already satisfied: sphinxcontrib-applehelp in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.0.2)\nRequirement already satisfied: sphinxcontrib-qthelp in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.0.3)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (0.7.12)\nRequirement already satisfied: snowballstemmer>=1.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2.0.0)\nRequirement already satisfied: babel>=1.3 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2.8.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.0.3)\nRequirement already satisfied: requests>=2.5.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2.23.0)\nRequirement already satisfied: sphinxcontrib-devhelp in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.0.2)\nRequirement already satisfied: sphinxcontrib-serializinghtml in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.1.4)\nRequirement already satisfied: imagesize in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.2.0)\nRequirement already satisfied: Jinja2>=2.3 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2.11.1)\nRequirement already satisfied: sphinxcontrib-jsmath in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.0.1)\nRequirement already satisfied: Pygments>=2.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2.5.2)\nRequirement already satisfied: pyparsing in /Users/crxon/558/env/lib/python3.7/site-packages (from rdflib>=4.2.2->ampligraph->-r requirements.txt (line 5)) (2.4.6)\nRequirement already satisfied: isodate in /Users/crxon/558/env/lib/python3.7/site-packages (from rdflib>=4.2.2->ampligraph->-r requirements.txt (line 5)) (0.6.0)\nRequirement already satisfied: decorator>=4.3.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from networkx>=2.3->ampligraph->-r requirements.txt (line 5)) (4.4.2)\nRequirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from flake8>=3.7.7->ampligraph->-r requirements.txt (line 5)) (0.3)\nRequirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from flake8>=3.7.7->ampligraph->-r requirements.txt (line 5)) (2.1.1)\nRequirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from flake8>=3.7.7->ampligraph->-r requirements.txt (line 5)) (2.5.0)\nRequirement already satisfied: mccabe<0.7.0,>=0.6.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from flake8>=3.7.7->ampligraph->-r requirements.txt (line 5)) (0.6.1)\nRequirement already satisfied: oset>=0.1.3 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinxcontrib-bibtex>=0.4.0->ampligraph->-r requirements.txt (line 5)) (0.1.3)\nRequirement already satisfied: pybtex-docutils>=0.2.0 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinxcontrib-bibtex>=0.4.0->ampligraph->-r requirements.txt (line 5)) (0.2.2)\nRequirement already satisfied: pybtex>=0.20 in /Users/crxon/558/env/lib/python3.7/site-packages (from sphinxcontrib-bibtex>=0.4.0->ampligraph->-r requirements.txt (line 5)) (0.22.2)\nRequirement already satisfied: zipp>=0.5 in /Users/crxon/558/env/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=3.5.1->ampligraph->-r requirements.txt (line 5)) (3.1.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /Users/crxon/558/env/lib/python3.7/site-packages (from requests>=2.5.0->sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/crxon/558/env/lib/python3.7/site-packages (from requests>=2.5.0->sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2019.11.28)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/crxon/558/env/lib/python3.7/site-packages (from requests>=2.5.0->sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.25.8)\nRequirement already satisfied: idna<3,>=2.5 in /Users/crxon/558/env/lib/python3.7/site-packages (from requests>=2.5.0->sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (2.9)\nRequirement already satisfied: MarkupSafe>=0.23 in /Users/crxon/558/env/lib/python3.7/site-packages (from Jinja2>=2.3->sphinx>=2.2->ampligraph->-r requirements.txt (line 5)) (1.1.1)\nRequirement already satisfied: latexcodec>=1.0.4 in /Users/crxon/558/env/lib/python3.7/site-packages (from pybtex>=0.20->sphinxcontrib-bibtex>=0.4.0->ampligraph->-r requirements.txt (line 5)) (2.0.0)\n"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity check:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'1.3.1'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ampligraph\n",
    "\n",
    "ampligraph.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "We will use the Game of Thrones (reduced) Knowledge Graph found in file `GoT.csv`.<br />\n",
    "Each relation (i.e. a triple) is in the form:`<subject, predicate, object>`\n",
    "\n",
    "Run the following cell to load the dataset in memory with using the `load_from_csv()` utility function:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import load_from_csv\n",
    "\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top triples:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([['Smithyton', 'SEAT_OF', 'House Shermer of Smithyton'],\n       ['House Mormont of Bear Island', 'LED_BY', 'Maege Mormont'],\n       ['Margaery Tyrell', 'SPOUSE', 'Joffrey Baratheon']], dtype=object)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X[:3, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list the subject and object entities found in the dataset:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['Abelar Hightower', 'Acorn Hall', 'Addam Frey', ..., 'the Antlers',\n       'the Paps', 'unnamed tower'], dtype=object)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "entities = np.unique(np.concatenate([X[:, 0], X[:, 2]]))\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and all of the relationships that link them."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array(['ALLIED_WITH', 'BRANCH_OF', 'FOUNDED_BY', 'HEIR_TO', 'IN_REGION',\n       'LED_BY', 'PARENT_OF', 'SEAT_OF', 'SPOUSE', 'SWORN_TO'],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "relations = np.unique(X[:, 1])\n",
    "relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks 1.1/1.2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.x.1 Defining train and test datasets\n",
    "\n",
    "As is typical in machine learning, we need to split our dataset into training and test sets.\n",
    "\n",
    "What differs from the standard method of randomly sampling N points to make up our test set, is that our data points are two entities linked by some relationship, and we need to take care to <u>ensure that all entities are represented in train and test sets by at least one triple</u>.\n",
    "\n",
    "To accomplish this, `AmpliGraph` provides the `train_test_split_no_unseen` function."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "# TODO: Split to training and data sets (according to task requriment),\n",
    "#       Please note that: - X.shape[0] gives the total number of rows in X\n",
    "#                         - the example code below creates a test set of size 100 (rows)\n",
    "\n",
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "\n",
    "X_train, X_test = train_test_split_no_unseen(X, test_size=int(X.shape[0]*0.1))\n",
    "# X_train, X_test = train_test_split_no_unseen(X, test_size=int(X.shape[0]*0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now split into train/test sets:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train set size:  (2858, 3)\nTest set size:  (317, 3)\n"
    }
   ],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.x.2 Training a model\n",
    "\n",
    "`AmpliGraph` has implemented several Knoweldge Graph Embedding models (*TransE, ComplEx, DistMult, HolE*). We will be using the *ComplEx* model (with default values), so lets import that:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import ComplEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go through the parameters to understand what's going on:\n",
    "- **k**: the dimensionality of the embedding space\n",
    "- **eta ($\\eta$)**: the number of negative, or false triples that must be generated at training runtime for each positive, or true triple\n",
    "- **batches_count**: the number of batches in which the training set is split during the training loop. If you are having into low memory issues than settings this to a higher number may help.\n",
    "- **epochs**: the number of epochs to train the model for.\n",
    "- **optimizer**: the Adam optimizer, with a learning rate of 1e-3 set via the *optimizer_params* kwarg.\n",
    "- **loss**: pairwise loss, with a margin of 0.5 set via the *loss_params* kwarg.\n",
    "- **regularizer**: $L_p$ regularization with $p=2$, i.e. l2 regularization. $\\lambda$ = 1e-5, set via the *regularizer_params* kwarg.\n",
    "\n",
    "Now we can instantiate the model:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplEx(batches_count=100, \n",
    "                seed=0, \n",
    "                epochs=200, \n",
    "                k=150, \n",
    "                eta=5,\n",
    "                optimizer='adam', \n",
    "                optimizer_params={'lr':1e-3},\n",
    "                loss='multiclass_nll', \n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':3, 'lambda':1e-5}, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering negatives\n",
    "\n",
    "`AmpliGraph` aims to follow `scikit-learn`'s ease-of-use design philosophy and simplify everything down to `fit`, `evaluate`, and `predict` functions.\n",
    "\n",
    "However, there are some knowledge graph specific steps we must take to ensure our model can be trained and evaluated correctly. The first of these is defining the filter that will be used to ensure that no *negative* statements generated by the corruption procedure are actually positives. This is simply done by concatenating our train and test sets. Now when negative triples are generated by the corruption strategy, we can check that they aren't actually true statements."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_filter = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Once you run the next cell the model will train:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Average Loss:   0.016231: 100%|██████████| 200/200 [02:10<00:00,  1.53epoch/s]\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.x.3 Evaluating a model\n",
    "\n",
    "Now it's time to evaluate our model on the test set to see how well it's performing.\n",
    "\n",
    "For this we'll use the `evaluate_performance` function:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import evaluate_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the arguments to this function:\n",
    "\n",
    "- `X`: the data to evaluate on. We're going to use our test set to evaluate.\n",
    "- `model`: the model we previously trained.\n",
    "- `filter_triples`: will filter out the false negatives generated by the corruption strategy.\n",
    "- `use_default_protocol`: specifies whether to use the default corruption protocol. If True, then subj and obj are corrupted separately during evaluation.\n",
    "- `verbose`: will give some nice log statements. Let's leave it on for now.\n",
    "\n",
    "Let's run some evaluations:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n100%|██████████| 317/317 [00:03<00:00, 96.03it/s]\n"
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,\n",
    "                             use_default_protocol=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ranks` returned by the `evaluate_performance` function indicate the rank at which the test set triple was found when performing link prediction using the model.\n",
    "\n",
    "<u>For example</u>, if we run the triple `<House Stark of Winterfell, IN_REGION, The North>` and the model returns a rank of `7`, it tells us that while it's not the highest likelihood true statement (which would be given a rank 1), it's pretty likely."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation metrics, we are going to use the `mrr_score` (mean reciprocal rank) and `hits_at_n_score` functions:\n",
    "- `mrr_score`: The function computes the mean of the reciprocal of elements of a vector of rankings ranks.\n",
    "- `hits_at_n_score`: The function computes how many elements of a vector of rankings ranks make it to the top n positions."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MRR: 0.35\nHits@10: 0.47\nHits@3: 0.38\nHits@1: 0.29\n"
    }
   ],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hits@N` indicates how many times in average a true triple was ranked in the top-N. The choice of which N makes more sense depends on the application. The Mean Reciprocal Rank (`MRR`) is another popular metrics to assess the predictive power of a model.\n",
    "\n",
    "**^ Please note that a screenshot of these scores are required for both task 1.1 and 1.2 ^**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.x.4 Predicting New Links\n",
    "\n",
    "Link prediction allows us to infer missing links in a graph. This has many real-world use cases, such as predicting connections between people in a social network, interactions between proteins in a biological network, and music recommendation based on prior user taste.\n",
    "\n",
    "In our case, we are going to see which of the following candidate statements are more likely to be true:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unseen = np.array([\n",
    "    ['Jorah Mormont', 'SPOUSE', 'Daenerys Targaryen'],\n",
    "    [\"King's Landing\", 'SEAT_OF', 'House Lannister of Casterly Rock'],\n",
    "    ['Brienne of Tarth', 'SPOUSE', 'Jaime Lannister'],\n",
    "    ['House Stark of Winterfell', 'IN_REGION', 'The North'],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 4/4 [00:00<00:00, 35.00it/s]\n"
    }
   ],
   "source": [
    "ranks_unseen = evaluate_performance(\n",
    "    X_unseen, \n",
    "    model=model, \n",
    "    filter_triples=unseen_filter,\n",
    "    corrupt_side = 's+o',\n",
    "    use_default_protocol=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the scores (real numbers) into probabilities (bound between 0 and 1) using the `expit` transform (note that the probabilities are not calibrated)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "probs = expit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           statement  rank     score      prob\n2            Brienne of Tarth SPOUSE Jaime Lannister  2430 -0.124812  0.468837\n0            Jorah Mormont SPOUSE Daenerys Targaryen  2232  0.039165  0.509790\n1  King's Landing SEAT_OF House Lannister of Cast...  1331  0.196228  0.548900\n3      House Stark of Winterfell IN_REGION The North   177  1.308594  0.787278",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>statement</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>Brienne of Tarth SPOUSE Jaime Lannister</td>\n      <td>2430</td>\n      <td>-0.124812</td>\n      <td>0.468837</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Jorah Mormont SPOUSE Daenerys Targaryen</td>\n      <td>2232</td>\n      <td>0.039165</td>\n      <td>0.509790</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>King's Landing SEAT_OF House Lannister of Cast...</td>\n      <td>1331</td>\n      <td>0.196228</td>\n      <td>0.548900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>House Stark of Winterfell IN_REGION The North</td>\n      <td>177</td>\n      <td>1.308594</td>\n      <td>0.787278</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
    "                      ranks_unseen, \n",
    "                      np.squeeze(scores),\n",
    "                      np.squeeze(probs))), \n",
    "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings with Tensorboard projector\n",
    "\n",
    "we can now visualize the high-dimensional embeddings in the browser. Lets import the `create_tensorboard_visualization` function, which simplifies the creation of the files necessary for Tensorboard to display the embeddings."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll run the function with our model, specifying the output path:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, 'inf558_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, we should now have a number of files a directory called `inf558_embeddings`.\n",
    "To visualize the embeddings in Tensorboard, go to (`cd`) `inf558_embeddings` and run the following command: `tensorboard --logdir=./visualizations`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Please note that a screenshot of embedding visualization is required for task 1.3 ^**"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}